{
  
    
        "post0": {
            "title": "Transformers Paper - Summary",
            "content": "Introduction . I aim to summarize the Attention Is All You Need paper after reading various blogposts and watching paper summary videos. . Machine Language Translation(MLT) Task . Lets start off by taking the example of Machine Language Translation(MLT) Task. Traditionally the approach has been to use RNNs (LSTMs) for this task. . Source-Sentence -&gt; Encoder -&gt; Embeddings -&gt; Decoder -&gt; Dest-Sentence . Source sentence is passed through an RNN Encoder, where the last hidden state is picked as an embeddings and then passed to a Decoder which finally outputs the translated sentence. . Attention Mechanism and Issues with RNNs . Attention mechanism was introduced to improve the performance of RNNs. . Issues with RNNs were : . Due to the sequentual nature of RNNs, long-range dependences were tough to capture. | A word goes through multiple hidden states for the translation task. | . How Attention Helps ? . With Attention, the decoder can go back and look into different aspects of the input. | A decoder can attend to the hidden states of the input sentence. ( h0, h1, h2, h3 .. ) | Pathway to input is much shorter rather than going through all the inputs. | . Basic idea is that a decoder in each step would output a bunch of keys. [d] - k1 , k2, k3 .. …. Kn ( these keys would index the hidden states via softmax architecture ) . Transformer Architecture . The paper proposes the Transformer architecture which has two components : . Encoder | Decoder | . The Source-Sentence goes in the Inputs and the part of the sentence translated till now goes in the Outputs part. . Every step in producing the next output is one training sample. | No multi-step backpropagation as in RNNs. | . Components of the Transformer : . Input and Output Embeddings are symmetrical. | Position Encoding : Encodes where the words are i.e positions of the words. | . Attention Blocks . There are a total of 3 attention blocks in the model : . One Attention Block in the Encoder : This block picks and chooses which words to look at. | Two Attention blocks in the Decoder : . The second attention block is interesting since it combines both the Source Sentence and the Target Sentence produced so far. . 3 connections go into it, 2 from the Encoder, and one from the decoder. . | . The 3 connections are : . Keys(K) - Output of the encoding part of Source Sentence | Values(V) - Output of the encoding part of Source Sentence | Queries(Q) - Output of the encoding part of Target Sentence | . Q and K have a dot product. In most cases in high dimensions, they will be at 90 degrees and their dot-products will be zeroes. But if the vectors are aligned ( in the same direction), their dot-product will be high ( non-zero). Dot-product is basically the angle between two vectors. . . We have a bunch of keys and each key has an associated value. . We compute the dot-product of the Queries(Q) with each of the keys and then a softmax over it. | We select the Key which aligns well with the Query and we multiply the softmax with the Values. | . softmax(&lt;K|Q&gt;) is a kind of indexing scheme to pick the appropriate value. . Q - I would like to know certain things. K - They are indexes. V - They have the attributes. . Intuition . The Encoder of the source sentence discovers interesting things and then builds key-value pairs. | The Encoder of the target sentence builds Queries. (Q) | Together they give the next signal. | . High Level Summary . Attention reduces the path length and is one of the main reasons why it should work better. | Attention mechanism helps reduce the amount of computation steps that information has to flow from one point of the network to another. | . Understanding Self-Attention . Step 1 . Encoders Input Vectors - A vector for each word . For each word, we create a : . Query vector | Key vector | Value vector | . Vectors are created by multiplying the embedding with 3 matrices that we trained which are W(q), W(k), W(v). . Input dimensions : 512 | Output : 64 | . . Multiplying X1 by the WQ weight matrix produces q1, the “query” vector associated with that word. | We end up creating a “query”, a “key”, and a “value” projection of each word in the input sentence. | . Step 2 . To calculate a score. | We score each word of the input sentence with every other word in the sentence. | . . If there are n words in a sentences with keys (k1, k2, k3 ….. kn) then the score is calculated for each word as follows : . –&gt; q1 * k1, q1 * k2, q1 * k3 …. q1 * kn . Matrix Calculation of Self-Attention . Packing our embeddings into a matrix X. | Multiplying it by weight matrices, WQ, WK, WV. | . . Understanding Multi-Headed-Attention . Multi headed means we have multiple sets of Q/K/V weight matrices. Transformer uses 8 sets for each encoder/decoder : . Used to project the input embeddings into a different representation subspace. | . . Diagram below summarizes the complete multi-headed attention process. . . Representing The Order of the Sequence Using Positional Encoding . Add a vector to each input embedding. | These vectors follow a specific pattern that the model learns, which helps it determine the exact position of each word. | . Other Aspects . There is a residual block to each of the encoder blocks. | Difference in the Decoder’s Attention Block : Self attention layer is only allowed to attend to earlier positions in the output sequence. | Future positions are masked(by setting to -inf) before the softmax step. | . | The Linear Layer projects decoder output to vector embedding which is equal to the vocabulary size. | . References . The first half of this blog post are notes from Yannic’s video explanation and the remaining parts are taken from The Illustrated Transformer blog. . Video Explanation by Yannic Kilcher : YouTube Link | The Illustrated Transformer by Jay Alammar : Blog | Attention Is All You Need paper. | The Annotated Transformer by harvardnlp is also a great resource which is a “annotated” version of the paper in the form of a line-by-line code implementation. |",
            "url": "https://dnaveenr.github.io/blog/2021/11/04/transformers.html",
            "relUrl": "/2021/11/04/transformers.html",
            "date": " • Nov 4, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Little Pieces that made the Whole",
            "content": "Introduction . I was part of the CVIT Summer School 2021, in one of the lectures by Prof.Vineeth Balasubramanian, the following slide was used to describe the various pieces which led to the advancements in DL. . Little Pieces that made the Whole. Credits : Prof.Vineeth Balasubramanian slides All the concepts mentioned in this slide are very important and the building blocks of a NN. These concepts are asked quite often in interviews, and I aim to write an overview of the concepts mentioned here for a quick glance and revision. . Regularization . Regularization is the concept of adding an additional term or penalty to your loss function such that it makes the model harder to learn the existing concept. It is often a common method used to solve the problem of overfitting since the weight added slows the weight update process and makes it harder to learn and not overfit. . 1. Dropout . . Dropout is a commonly used method in deep learning. In Dropout, you randomly drop a set of neurons in your neural network based on a probability metric. So the network is made to learn with a lesser number of neurons or weights which makes it learn better, more robust, and not overfit on the data. . . Dropout drops the activations in the network. | Dropout is used only in the training process and not during test time. | . 2. DropConnect . . DropConnect is similar to Dropout, but it drops the weights instead of the nodes in the network with a certain probability, so a node remains partially active. . Both Dropout and DropConnect are methods used to prevent co-adaption in the network. This means that we want the units to independently learn rather than depending on each other. In each training step, the weights or activations drop will be different. . 3. Batch Normalization . In Batch Normalization(BN), we take the average of the mean and standard deviations of the activations of the layers and use these to normalize the activations. This way the distribution of the neurons after each layer remains the same, thus boosting the performance of the model, speeds up training, and gives us the ability to use a higher learning rate. . BN is applied for every training mini-batch. It is believed that BN adds a sense of randomness since each mini-batch will have a different set of means and standard deviations, thus the normalized values will be slightly different each time. . . If the normalized activations are y^. BN Layer would contain: gamma * y^ + beta, where gamma, beta are learnable parameters. These are used to tune the normalized activations to make accurate predictions. | . 4. Data Augmentation . Data augmentation is the process of : . adding variety to your dataset by having variations to your data, | helps in increasing your dataset size. | helps in making the model more robust and prevents overfitting to some extent. | . Examples of common data augmentation techniques for images are rotation, flipping, perspective warping, brightness changes, and contrast changes. Examples for text include back translation, synonym replacement, random insertion etc. . 5. Noise in Data/Label/Gradient . It has been observed that adding random noise helps in generalisation, fault tolerance, and better learning. Adding noise has a regularization effect. . The ways we can add noise include: . adding noise to training data ( a form of data augmentation ) | adding noise to labels ( class labels ) | adding gradient noise | . . Gaussian noise is added to every gradient g at every time step t. . Weight Initialisation . Weight initialisation is the concept of how the weights in the model are initialised before the training is started. It plays a huge impact on how well the model learns and has an effect on the final accuracy as well. . 1. Xavier’s initialisation . The goal of this intialisation is to ensure that the variance of activations is the same across all layers. The constant variance helps in avoiding the gradient from exploding or vanishing. . Xavier init is designed to work well with tanh and sigmoid activations. | . Xavier initialization sets a layer’s weights to values chosen from a random uniform distribution that’s bounded between : . In the above equation: . nj is the number of incoming connections to a layer. | nj+1 is the number of outgoing connections from a layer. | . 2. He’s initialisation . This is more suited for activation functions such as ReLU, it takes into account the non-linearity of functions. It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / fan_in) where fan_in is the number of input units in the weight tensor. . Choosing Gradient Descent Parameters . We discuss the optimizers are used in the gradient descent process. These help in training the network better and reaching the global minima faster. . 1. Adagrad : Adaptive Gradient Optimizer . w&#39; = w - alpha * dw . Typically in GD, mini-batch GD, the learning rate is fixed. . Core idea : each weight has a different learning rate. It is parameter-based learning rate tuning. . More updates a parameter gets, smaller the weightage of the update. [low learning rate for parameters of frequently occurring features] | Fewer updates a parameter gets, more the weightage of the update. [high learning rate for parameters for infrequently occurring features] | . It is well suited for working with sparse data. . . where Gt holds the sum of the squares of all previous gradients till the point t. . Advantages : . No need to manually change the learning rate. | . Disadvantages : . Accumulation of squared gradient in the denominator. It keeps increasing with training and thus leads to making the learning rate too small with time. | . 2. RMSProp - Root Mean Squared . It is very similar to Adagrad but it aims to fix the problem of the diminishing gradient by using an exponentially decaying average of the gradient. . . For example, in the above equation . if value of beta = 0.9, | vt will be 0.9 * previous gradient + 0.1 * (current_gradient) ^ 2 | . This means the direction of the previous gradients is given more weightage than the recent update. This ensures that the gradient does not oscillate much and moves in the right direction. . . 3. Adam - Adaptive Moment Estimation . The addition in Adam is that it stores : . exponentially decaying average of past gradients (mt) - Equation 1 + | exponentially decaying average of past squared gradients (vt) - Equation 2 [As in RMSProp] | . . The authors observed that when : . mt, vt are initialised as vectors of zeros and | when B1, B2 are small ( close to 1) mt, vt were biased towards zeros | . | . To counter these biases, they compute bias-corrected moments i.e equation 3 and 4 in the above figure. . Equation 5 is the final Adam equation. . Authors propose the following default values : . B1 - 0.9 | B2 - 0.999 | Epsilon - 10^-8 | Good default suggested for Learning rate - n - 0.001 | . According to Kingma et al., 2014, the Adam optimizer is “computationally efficient, has little memory requirement, invariant to diagonal rescaling of gradients, and is well suited for problems that are large in terms of data/parameters”. . 4. Momentum . Momentum helps in reaching the minima faster by giving a push by making the gradient move in the direction of the previous update. It results in faster convergence and reduced oscillations. . Equation : w = w - n * dw + gamma * vt where n - the learning rate vt - is the value of w at time t-1 (last update to w) gamma - the momentum parameter (usually initialised to 0.9) . This gist of momentum is that we get to local minima faster because we don’t oscillate up and down the y-axis. The time to convergence is faster when using momentum. . Pros : . faster convergence than SGD Cons : | If momentum is too high, then we may miss the minima and move ahead, then moving backwards and end up missing it again. | . 5. Nesterov Momentum (look-ahead momentum) . It is a kind of look-ahead momentum updation. This is done by using an approximation of the next gradient value in the present momentum update equation. . w_ahead = w + gamma * vt-1 [n*dw is ignored] (Gives an approximation of the next position of parameters, refer momentum eq.) vt = gamma * vt-1 - n * dw_ahead w = w + vt . The gradient is computed at the lookahead point(w_ahead) instead of the old position w. . . Nesterov momentum: Instead of evaluating the gradient at the current position (red circle), we know that our momentum is about to carry us to the tip of the green arrow. With Nesterov momentum, we therefore instead evaluate the gradient at this “looked-ahead” position. . Activation Functions . An activation function is a non-linear transformation we do over the weighted sum of inputs before sending it to the next layer. . . Activation functions add non-linearity to our NN and without it, our NN would just be a product + bias addition which would be a linear transformation. This would limit the capabilities of our NN. . 1. ReLU - Rectified Linear Unit . ReLU function : f(x) = 0 , x &lt; 0 = x when x &gt; 0 -&gt; f(x) = max(0, x) . . ReLU helps in faster and efficient training of the NN and leads to fewer vanishing gradient problems. . Issues : . Non-differentiable at zero. | Not zero-centered | Unbounded | Neurons can be pushed into states in which they become inactive for essentially all inputs. This is called the dying ReLU problem. | . 2. Leaky ReLUs . Modified version of ReLU f(x) = x , when x &gt; 0 = 0.01 * x, when x &lt; 0 . Leaky ReLUs allow a small, positive gradient when the unit is not active. This activation is preferred in tasks that suffer from sparse gradients, for example in training GANs. . . 3. PReLU - Parametric ReLU . The generalized version of Leaky ReLU, where a parameter is used instead of 0.01. . Parametric ReLU f(x) = x , x &gt; 0 alpha * x, x &lt; 0 . Alpha is learned along with other NN parameters. The intuition is that different layers may require different types of non-linearity. . 4. ELU - Exponential Linear Units . ELU function : f(x) = x , x &gt; 0 a*(e^x - 1) , otherwise a is a hyperparameter to be tuned, and a &gt; 0 is a constraint. . . ELUs try to make the mean activations closer to zero, which speeds up the learning. . Loss Functions . A loss function is a function we use to optimize the parameters of our model. . 1. Cross Entropy Loss . It is generally used for the optimization of classification problems. When the number of classes is 2, it is known as binary cross entropy loss. . . 2. Embedding Loss . The embedding loss functions are a class of loss functions used in deep metric learning. These are used to improve the embedding generated so that similar inputs have a closer distance and dissimilar inputs have a larger distance. Examples of embedding losses are contrastive loss, triplet loss, margin loss etc. . 3. Mean-Squared Loss . The average squared error of the actual and estimated value i.e mean of the square of the errors. . . It is a commonly used loss function for regression. . 4. Absolute Error . The sum of the absolute difference between the actual and estimated value. . Absolute Error : |y-y_| where y is the actual value y_ is the estimated value . It is a loss function used to regression models . 5. KLDivergence . It is the measure of how two probability distributions (eg: p and q ) are different from each other. . KLDivergence : D(P || Q) = summation p(x) * log(p(x)/q(x)) . As a loss function, the divergence loss between y_true and y_pred is calculated. . Loss(y_true || y_pred) = y_true * log(y_true/y_pred) . 6. Max-Margin Loss . It is also known as hinge loss. It is used for training SVMs for the task of classification. . Loss = max(1 - y_true * y_pred, 0) y_true are expected to be -1 or 1. . References . In writing this post, I have referred various articles, blog posts, the FastAI book to get a better understanding. I am adding some references to read deeper on some of the topics : . Optimizers : Great blog post by Sebastian Ruder on “An overview of gradient descent optimization algorithms”. | ML from Scratch - Link | CS231n Course - Link | . | Embedding Loss : Explanation of the various embedding loss functions - Link | . | FastAI Book - Link | . Do comment and let me know if you have any feedback or suggestions to improve this blogpost. Thank you! .",
            "url": "https://dnaveenr.github.io/blog/2021/09/20/pieces-that-made-ml.html",
            "relUrl": "/2021/09/20/pieces-that-made-ml.html",
            "date": " • Sep 20, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "The Drivetrain Approach",
            "content": "Introduction . The drivetrain approach is introduced by Jeremy Howard along with Margit Zwemer and Mike Loukidesin in his book “Designing Great Data Products”. . I came across this approach in the FastAI Book and sharing my understanding of the topic here. . Goal of the Approach . The main goal of the Drivetrain approach is to produce actionable outcomes from models. This means the results from a model must help and add value to your task. . Approach . . The four steps in the Drivetrain Approach. Credits: OReilly The approach is explained as a 4 step process : . Have a clear objective. What are you trying to achieve ? | . | Levers - Actions to be taken to achieve the objective. What is a lever ? By definition, a lever is a handle or bar that is attached to a piece of machinery and which you push or pull in order to operate the machinery. Okay ? But in business terms, it means “initiatives” that are taken to drive the desired impact. | Data What data do we have that we can use ? | What data needs to be collected to achieve the objective ? | . | Models - Building the models Finally, build a model that we can use to determine the best actions to take to get the best results in terms of the objective. | The first three steps are important and need to be worked on before proceeding to the last step of building the models. These steps will help you achieve your objective and also lead to generating actionable outcomes from your models which can further be used in the system. . Example - Logo Detection System . Objective . It would be to detect all logos in a give image. . Levers . Get a team of taggers to annotate logos. ( In-house or outsource ?) | Determine the environment where we will deploying so as to choose the appropriate model for latency and compute requirements. | Is there any pre-trained model we can start off with for this task or do we need to train from scratch ? | . Data . What all in-house data of logos or images containing logos do we have ? | Which all publicly available datasets can we use ? | Do we need to scrap data from the internet for this task ? | . Model . Train a logo detection model keeping the levers, data and objective in mind, so we can finally make an actionable outcome. | . Ending . This is small example I have taken for a problem to understand and use the drivetrain approach. I have presented each step with a series of questions we may have and thereby answering these questions would lead to better understanding of the problem. . If you have any thoughts or questions to ask, comment in the section below. .",
            "url": "https://dnaveenr.github.io/blog/drivetrain/ml/approach/2021/01/02/drive-train-approach.html",
            "relUrl": "/drivetrain/ml/approach/2021/01/02/drive-train-approach.html",
            "date": " • Jan 2, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a machine learning engineer at Sensara Technologies, where I work on image/video content analysis using computer vision and deep learning techniques. . At Sensara1, I’ve worked on problems such as channel logo recognition, image quality analysis, object detection/recognition, face recognition, deep metric learning, image similarity, on-device machine learning. 2. (2018 - Present) . I did my bachelors in Computer Science &amp; Engineering at PES University, Bangalore. (2014-2018) . Part from work, I enjoy reading books, running, traveling and trekking. I go for treks quite often and love being close to the nature. . Sensara Technologies India &#8617; . | Project details here : dnaveenr.github.io &#8617; . |",
          "url": "https://dnaveenr.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  
      ,"page5": {
          "title": "Travel",
          "content": "Coming soon. .",
          "url": "https://dnaveenr.github.io/blog/travel/",
          "relUrl": "/travel/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://dnaveenr.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}